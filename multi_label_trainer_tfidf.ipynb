{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi-label-trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9ZG7WNCHNqw"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "In this example, we will build a multi-label text classifier to predict the subject areas of arXiv papers from their abstract bodies. This type of classifier can be useful for conference submission portals like [OpenReview](https://openreview.net/). Given a paper abstract, the portal could provide suggestions on which areas the underlying paper would best belong to.\n",
        "\n",
        "The dataset was collected using the [`arXiv` Python library](https://github.com/lukasschwab/arxiv.py) that provides a wrapper around the [original arXiv API](http://arxiv.org/help/api/index). To know more, please refer to [this notebook](https://github.com/soumik12345/multi-label-text-classification/blob/master/arxiv_scrape.ipynb). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GICQpY-zws7"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho5uPff1fLoH"
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ast import literal_eval\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyeQtKSezymP"
      },
      "source": [
        "## Read data and perform basic EDA\n",
        "\n",
        "In this section, we first load the dataset into a `pandas` dataframe and then perform some basic exploratory data analysis (EDA)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "yFo2pNYbf2Du",
        "outputId": "58256e1b-f8dd-4ae9-b9d0-34c3ddf09ae8"
      },
      "source": [
        "arxiv_data = pd.read_csv(\n",
        "    \"https://github.com/soumik12345/multi-label-text-classification/releases/download/v0.2/arxiv_data.csv\"\n",
        ")\n",
        "arxiv_data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              titles  ...                        terms\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...  ...           ['cs.CV', 'cs.LG']\n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...  ...  ['cs.CV', 'cs.AI', 'cs.LG']\n",
              "2  Enforcing Mutual Consistency of Hard Regions f...  ...           ['cs.CV', 'cs.AI']\n",
              "3  Parameter Decoupling Strategy for Semi-supervi...  ...                    ['cs.CV']\n",
              "4  Background-Foreground Segmentation for Interio...  ...           ['cs.CV', 'cs.LG']\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djk-kXWvHNq3"
      },
      "source": [
        "Our text features are present in the `summaries` column and their corresponding labels are in `terms`. As we can notice there are multiple categories associated with a particular entry. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Em_8mJvUgKY-",
        "outputId": "0989c06c-7aad-4f11-dc64-1104d9a42acb"
      },
      "source": [
        "print(f\"There are {len(arxiv_data)} rows in the dataset.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 51774 rows in the dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3bYjP_0HNq4"
      },
      "source": [
        "Real-world data is noisy. One of the most commonly observed such noise is data duplication. Here we notice that our initial dataset has got about 13k duplicate entries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9Vb9jtK5zjg",
        "outputId": "a7916845-ddfa-416e-8517-7254e5fe9776"
      },
      "source": [
        "total_duplicate_titles = sum(arxiv_data[\"titles\"].duplicated())\n",
        "print(f\"There are {total_duplicate_titles} duplicate titles.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 12802 duplicate titles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEBbarGSHNq5"
      },
      "source": [
        "Before proceeding further we first drop these entries. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2259X-rf6OLY",
        "outputId": "7d9c4f97-ad8b-4e83-f547-7395728c54ef"
      },
      "source": [
        "arxiv_data = arxiv_data[~arxiv_data[\"titles\"].duplicated()]\n",
        "print(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 38972 rows in the deduplicated dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmgkCCr2g0w5",
        "outputId": "d77c5153-6c6e-433a-c7a9-c5f2bbfc65dc"
      },
      "source": [
        "# There are some terms with occurrence as low as 1.\n",
        "print(sum(arxiv_data[\"terms\"].value_counts() == 1))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2321\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyBJNHMdifJ-",
        "outputId": "8c661802-98e8-4e32-eabf-00ba9f43b2d3"
      },
      "source": [
        "# How many unique terms?\n",
        "print(arxiv_data[\"terms\"].nunique())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPvNGph0HNq7"
      },
      "source": [
        "As observed above, out of 3157 unique combinations of `terms`, 2321 entries have the lowest occurrence. To prepare our train, validation, and test sets with [stratification](https://en.wikipedia.org/wiki/Stratified_sampling), we need to drop these terms. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77ZoCzrMhLxc",
        "outputId": "45d05470-af7a-49ae-f4f5-ab42c5c6f845"
      },
      "source": [
        "# Filtering the rare terms.\n",
        "arxiv_data_filtered = arxiv_data.groupby(\"terms\").filter(lambda x: len(x) > 1)\n",
        "arxiv_data_filtered.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36651, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxrG9tim0QNr"
      },
      "source": [
        "## Convert the string labels to list of strings\n",
        "\n",
        "The initial labels are represented as raw strings. Here we make them `List[str]` for a more compact representation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIEGLc61iwbQ",
        "outputId": "e328a97d-0d45-4fa0-e694-6c97de639424"
      },
      "source": [
        "arxiv_data_filtered[\"terms\"] = arxiv_data_filtered[\"terms\"].apply(\n",
        "    lambda x: literal_eval(x)\n",
        ")\n",
        "arxiv_data_filtered[\"terms\"].values[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list(['cs.CV', 'cs.LG']), list(['cs.CV', 'cs.AI', 'cs.LG']),\n",
              "       list(['cs.CV', 'cs.AI']), list(['cs.CV']),\n",
              "       list(['cs.CV', 'cs.LG'])], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjFB8Uoo0cXM"
      },
      "source": [
        "## Stratified splits because of class imbalance\n",
        "\n",
        "The dataset has a [class imbalance problem](https://developers.google.com/machine-learning/glossary/#class-imbalanced-dataset). So, to have a fair evaluation result, we need to ensure the datasets are sampled with stratification. To know more about different strategies to deal with the class imbalance problem, you can follow [this tutorial](https://www.tensorflow.org/tutorials/structured_data/imbalanced_data). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbKDVTKPgOKe",
        "outputId": "b9abccd7-5873-486f-94e3-fc52e72cec83"
      },
      "source": [
        "test_split = 0.1\n",
        "\n",
        "# Initial train and test split.\n",
        "train_df, test_df = train_test_split(\n",
        "    arxiv_data_filtered,\n",
        "    test_size=test_split,\n",
        "    stratify=arxiv_data_filtered[\"terms\"].values,\n",
        ")\n",
        "\n",
        "# Splitting the test set further into validation\n",
        "# and new test sets.\n",
        "val_df = test_df.sample(frac=0.5)\n",
        "test_df.drop(val_df.index, inplace=True)\n",
        "\n",
        "print(f\"Number of rows in training set: {len(train_df)}\")\n",
        "print(f\"Number of rows in validation set: {len(val_df)}\")\n",
        "print(f\"Number of rows in test set: {len(test_df)}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in training set: 32985\n",
            "Number of rows in validation set: 1833\n",
            "Number of rows in test set: 1833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py:4174: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  errors=errors,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96Ew2PPI0lVc"
      },
      "source": [
        "## Multi-label binarization\n",
        "\n",
        "Now we preprocess our labels using [`MultiLabelBinarizer`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MultiLabelBinarizer.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vgxbdwGf07E",
        "outputId": "197aa2d4-8e2e-4f3e-813a-bcd3ca0d8705"
      },
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "mlb.fit_transform(train_df[\"terms\"])\n",
        "mlb.classes_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['14J60 (Primary) 14F05, 14J26 (Secondary)', '62H30', '62H35',\n",
              "       '62H99', '65D19', '68', '68Q32', '68T01', '68T05', '68T07',\n",
              "       '68T10', '68T30', '68T45', '68T99', '68Txx', '68U01', '68U10',\n",
              "       'E.5; E.4; E.2; H.1.1; F.1.1; F.1.3', 'F.2.2; I.2.7', 'G.3',\n",
              "       'H.3.1; H.3.3; I.2.6; I.2.7', 'H.3.1; I.2.6; I.2.7', 'I.2',\n",
              "       'I.2.0; I.2.6', 'I.2.1', 'I.2.10', 'I.2.10; I.2.6',\n",
              "       'I.2.10; I.4.8', 'I.2.10; I.4.8; I.5.4', 'I.2.10; I.4; I.5',\n",
              "       'I.2.10; I.5.1; I.4.8', 'I.2.1; J.3', 'I.2.6', 'I.2.6, I.5.4',\n",
              "       'I.2.6; I.2.10', 'I.2.6; I.2.7', 'I.2.6; I.2.7; H.3.1; H.3.3',\n",
              "       'I.2.6; I.2.8', 'I.2.6; I.2.9', 'I.2.6; I.5.1', 'I.2.6; I.5.4',\n",
              "       'I.2.7', 'I.2.8', 'I.2; I.2.6; I.2.7', 'I.2; I.4; I.5', 'I.2; I.5',\n",
              "       'I.2; J.2', 'I.4', 'I.4.0', 'I.4.3', 'I.4.4', 'I.4.5', 'I.4.6',\n",
              "       'I.4.6; I.4.8', 'I.4.8', 'I.4.9', 'I.4.9; I.5.4', 'I.4; I.5',\n",
              "       'I.5.4', 'K.3.2', 'astro-ph.IM', 'cond-mat.dis-nn',\n",
              "       'cond-mat.mtrl-sci', 'cond-mat.soft', 'cond-mat.stat-mech',\n",
              "       'cs.AI', 'cs.AR', 'cs.CC', 'cs.CE', 'cs.CG', 'cs.CL', 'cs.CR',\n",
              "       'cs.CV', 'cs.CY', 'cs.DB', 'cs.DC', 'cs.DM', 'cs.DS', 'cs.ET',\n",
              "       'cs.FL', 'cs.GR', 'cs.GT', 'cs.HC', 'cs.IR', 'cs.IT', 'cs.LG',\n",
              "       'cs.LO', 'cs.MA', 'cs.MM', 'cs.MS', 'cs.NA', 'cs.NE', 'cs.NI',\n",
              "       'cs.PF', 'cs.PL', 'cs.RO', 'cs.SC', 'cs.SD', 'cs.SE', 'cs.SI',\n",
              "       'cs.SY', 'econ.EM', 'econ.GN', 'eess.AS', 'eess.IV', 'eess.SP',\n",
              "       'eess.SY', 'hep-ex', 'hep-ph', 'math.AP', 'math.AT', 'math.CO',\n",
              "       'math.DS', 'math.FA', 'math.IT', 'math.LO', 'math.NA', 'math.OC',\n",
              "       'math.PR', 'math.ST', 'nlin.AO', 'nlin.CD', 'physics.ao-ph',\n",
              "       'physics.bio-ph', 'physics.chem-ph', 'physics.comp-ph',\n",
              "       'physics.data-an', 'physics.flu-dyn', 'physics.geo-ph',\n",
              "       'physics.med-ph', 'physics.optics', 'physics.soc-ph', 'q-bio.BM',\n",
              "       'q-bio.GN', 'q-bio.MN', 'q-bio.NC', 'q-bio.OT', 'q-bio.QM',\n",
              "       'q-bio.TO', 'q-fin.CP', 'q-fin.EC', 'q-fin.GN', 'q-fin.PM',\n",
              "       'q-fin.RM', 'q-fin.ST', 'q-fin.TR', 'quant-ph', 'stat.AP',\n",
              "       'stat.CO', 'stat.ME', 'stat.ML', 'stat.TH'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTcmRnP9HNq9"
      },
      "source": [
        "`MultiLabelBinarizer`separates out the individual unique classes available from the label pool and then uses this information to represent a given label set with 0's and 1's. Below is an example. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfMO31s8HNq9",
        "outputId": "b490a76b-f1a7-4a9b-87ea-e08a72ca15b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sample_label = train_df[\"terms\"].iloc[0]\n",
        "print(f\"Original label: {sample_label}\")\n",
        "\n",
        "label_binarized = mlb.transform([sample_label])\n",
        "print(f\"Label-binarized representation: {label_binarized}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original label: ['cs.CV']\n",
            "Label-binarized representation: [[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2kFVBCG0oXV"
      },
      "source": [
        "## Data preprocessing and `tf.data.Dataset` objects\n",
        "\n",
        "We first get percentile estimates of the sequence lengths. The purpose will be clear in a moment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCR-_Iw3gyT-",
        "outputId": "9a9a3b56-73d4-4d4c-bf9a-9799cf607797"
      },
      "source": [
        "train_df[\"summaries\"].apply(lambda x: len(x.split(\" \"))).describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    32985.000000\n",
              "mean       156.551342\n",
              "std         41.456720\n",
              "min          5.000000\n",
              "25%        128.000000\n",
              "50%        155.000000\n",
              "75%        183.000000\n",
              "max        462.000000\n",
              "Name: summaries, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rdivJop02xG"
      },
      "source": [
        "Notice that 50% of the abstracts have a length of 154 (you may get a different number based on the split). So, any number near that is a good enough approximate for the maximum sequence length. \n",
        "\n",
        "Now, we write utilities to prepare our datasets that would go straight to the text classifier model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoMgEZrtVBDS"
      },
      "source": [
        "max_seqlen = 150\n",
        "batch_size = 128\n",
        "padding_token = \"<pad>\"\n",
        "\n",
        "\n",
        "def unify_text_length(text, label):\n",
        "    # Split the given abstract and calculate its length.\n",
        "    word_splits = tf.strings.split(text, sep=\" \")\n",
        "    sequence_length = tf.shape(word_splits)[0]\n",
        "    \n",
        "    # Calculate the padding amount.\n",
        "    padding_amount = max_seqlen - sequence_length\n",
        "    \n",
        "    # Check if we need to pad or truncate.\n",
        "    if padding_amount > 0:\n",
        "        unified_text = tf.pad([text], [[0, padding_amount]], constant_values=\"<pad>\")\n",
        "        unified_text = tf.strings.reduce_join(unified_text, separator=\"\")\n",
        "    else:\n",
        "        unified_text = tf.strings.reduce_join(word_splits[:max_seqlen], separator=\" \")\n",
        "    \n",
        "    # The expansion is needed for subsequent vectorization.\n",
        "    return tf.expand_dims(unified_text, -1), label\n",
        "\n",
        "\n",
        "def make_dataset(dataframe, train=True):\n",
        "    label_binarized = mlb.transform(dataframe[\"terms\"].values)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(\n",
        "        (dataframe[\"summaries\"].values, label_binarized)\n",
        "    )\n",
        "    if train:\n",
        "        dataset = dataset.shuffle(batch_size * 10)\n",
        "    dataset = dataset.map(unify_text_length).cache()\n",
        "    return dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi5g9v3HHNq-"
      },
      "source": [
        "Now we can prepare the `tf.data.Dataset` objects. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_vrgkCXrWOS"
      },
      "source": [
        "train_dataset = make_dataset(train_df)\n",
        "validation_dataset = make_dataset(val_df, False)\n",
        "test_dataset = make_dataset(test_df, False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1Bb4Xnm1EwK"
      },
      "source": [
        "## Dataset preview"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-8k7gScVoz6",
        "outputId": "b6e0d200-6149-42a4-a87e-1970246841aa"
      },
      "source": [
        "text_batch, label_batch = next(iter(train_dataset))\n",
        "\n",
        "for i, text in enumerate(text_batch[:5]):\n",
        "    label = label_batch[i].numpy()[None, ...]\n",
        "    print(f\"Abstract: {text[0]}\")\n",
        "    print(f\"Label(s): {mlb.inverse_transform(label)[0]}\")\n",
        "    print(\" \")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Abstract: b'Supply chain network data is a valuable asset for businesses wishing to\\nunderstand their ethical profile, security of supply, and efficiency.\\nPossession of a dataset alone however is not a sufficient enabler of actionable\\ndecisions due to incomplete information. In this paper, we present a graph\\nrepresentation learning approach to uncover hidden dependency links that focal\\ncompanies may not be aware of. To the best of our knowledge, our work is the\\nfirst to represent a supply chain as a heterogeneous knowledge graph with\\nlearnable embeddings. We demonstrate that our representation facilitates\\nstate-of-the-art performance on link prediction of a global automotive supply\\nchain network using a relational graph convolutional network. It is anticipated\\nthat our method will be directly applicable to businesses wishing to sever\\nlinks with nefarious entities and mitigate risk of supply failure. More\\nabstractly, it is anticipated that our method will be useful to inform\\nrepresentation learning of supply chain networks for downstream tasks beyond\\nlink prediction.<pad><pad><pad><pad>'\n",
            "Label(s): ('cs.LG',)\n",
            " \n",
            "Abstract: b'We consider the task of pixel-wise semantic segmentation given a small set of\\nlabeled training images. Among two of the most popular techniques to address\\nthis task are Decision Forests (DF) and Neural Networks (NN). In this work, we\\nexplore the relationship between two special forms of these techniques: stacked\\nDFs (namely Auto-context) and deep Convolutional Neural Networks (ConvNet). Our\\nmain contribution is to show that Auto-context can be mapped to a deep ConvNet\\nwith novel architecture, and thereby trained end-to-end. This mapping can be\\nused as an initialization of a deep ConvNet, enabling training even in the face\\nof very limited amounts of training data. We also demonstrate an approximate\\nmapping back from the refined ConvNet to a second stacked DF, with improved\\nperformance over the original. We experimentally verify that these mappings\\noutperform stacked DFs for two different applications in computer vision and\\nbiology: Kinect-based body part labeling from depth images, and somite\\nsegmentation in microscopy images of developing zebrafish. Finally,'\n",
            "Label(s): ('cs.CV',)\n",
            " \n",
            "Abstract: b'We consider the problem of semantic image segmentation using deep\\nconvolutional neural networks. We propose a novel network architecture called\\nthe label refinement network that predicts segmentation labels in a\\ncoarse-to-fine fashion at several resolutions. The segmentation labels at a\\ncoarse resolution are used together with convolutional features to obtain finer\\nresolution segmentation labels. We define loss functions at several stages in\\nthe network to provide supervisions at different stages. Our experimental\\nresults on several standard datasets demonstrate that the proposed model\\nprovides an effective way of producing pixel-wise dense image labeling.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
            "Label(s): ('cs.CV',)\n",
            " \n",
            "Abstract: b'Multivariate time series (MTS) data are becoming increasingly ubiquitous in\\ndiverse domains, e.g., IoT systems, health informatics, and 5G networks. To\\nobtain an effective representation of MTS data, it is not only essential to\\nconsider unpredictable dynamics and highly variable lengths of these data but\\nalso important to address the irregularities in the sampling rates of MTS.\\nExisting parametric approaches rely on manual hyperparameter tuning and may\\ncost a huge amount of labor effort. Therefore, it is desirable to learn the\\nrepresentation automatically and efficiently. To this end, we propose an\\nautonomous representation learning approach for multivariate time series\\n(TimeAutoML) with irregular sampling rates and variable lengths. As opposed to\\nprevious works, we first present a representation learning pipeline in which\\nthe configuration and hyperparameter optimization are fully automatic and can\\nbe tailored for various tasks, e.g., anomaly detection, clustering, etc. Next,\\na negative sample generation approach and an auxiliary classification task are\\ndeveloped and integrated within TimeAutoML to enhance its representation\\ncapability. Extensive empirical'\n",
            "Label(s): ('cs.AI', 'cs.LG', 'stat.ML')\n",
            " \n",
            "Abstract: b'Hashing produces compact representations for documents, to perform tasks like\\nclassification or retrieval based on these short codes. When hashing is\\nsupervised, the codes are trained using labels on the training data. This paper\\nfirst shows that the evaluation protocols used in the literature for supervised\\nhashing are not satisfactory: we show that a trivial solution that encodes the\\noutput of a classifier significantly outperforms existing supervised or\\nsemi-supervised methods, while using much shorter codes. We then propose two\\nalternative protocols for supervised hashing: one based on retrieval on a\\ndisjoint set of classes, and another based on transfer learning to new classes.\\nWe provide two baseline methods for image-related tasks to assess the\\nperformance of (semi-)supervised hashing: without coding and with unsupervised\\ncodes. These baselines give a lower- and upper-bound on the performance of a\\nsupervised hashing scheme.<pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>'\n",
            "Label(s): ('cs.CV',)\n",
            " \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yfbnoi-y1IoB"
      },
      "source": [
        "## Vocabulary size for vectorization\n",
        "\n",
        "Before we feed the data to our model we need to represent them as numbers. For that purpose, we will use the [`TextVectorization` layer](https://keras.io/api/layers/preprocessing_layers/text/text_vectorization). It can operate as a part of your main model so that the model is excluded from the core preprocessing logic. This greatly reduces the chances of training and serving skew. \n",
        "\n",
        "We first calculate the number of unique words present in the abstracts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIW42KOgyfE7",
        "outputId": "f09cac02-69a1-401e-c88a-4fe5c8f22bf3"
      },
      "source": [
        "train_df[\"total_words\"] = train_df[\"summaries\"].str.split().str.len()\n",
        "vocabulary_size = train_df[\"total_words\"].max()\n",
        "print(f\"Vocabulary size: {vocabulary_size}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 498\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nis2d-xFHNq_"
      },
      "source": [
        "Now we can create our text classifier model with the `TextVectorization` layer present inside it. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TBUFHm11M3G"
      },
      "source": [
        "## Create model with `TextVectorization`\n",
        "\n",
        "A batch of raw text will first go through the `TextVectorization` layer and it will generate their integer representations. These will then be passed to the shallow model responsible for text classification. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XX7ovyPokNs"
      },
      "source": [
        "text_vectorizer = layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n",
        ")\n",
        "\n",
        "# `TextVectorization` needs to be adapted as per the vocabulary from our\n",
        "# training set.\n",
        "with tf.device(\"/CPU:0\"):\n",
        "    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n",
        "\n",
        "\n",
        "def make_model():\n",
        "    shallow_mlp_model = keras.Sequential(\n",
        "        [\n",
        "            keras.Input(shape=(), dtype=tf.string),\n",
        "            text_vectorizer,\n",
        "            layers.Dense(512, activation=\"relu\"),\n",
        "            layers.Dense(256, activation=\"relu\"),\n",
        "            layers.Dense(len(mlb.classes_), activation=\"sigmoid\"),\n",
        "        ]\n",
        "    )\n",
        "    return shallow_mlp_model"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7i71RLtugUs"
      },
      "source": [
        "Without the CPU placement, we run into: \n",
        "\n",
        "```\n",
        "(1) Invalid argument: During Variant Host->Device Copy: non-DMA-copy attempted of tensor type: string\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58tUdCsQuI1P",
        "outputId": "362fcc18-0ae2-4f33-9d9f-cc94399dda55"
      },
      "source": [
        "shallow_mlp_model = make_model()\n",
        "shallow_mlp_model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "text_vectorization (TextVect (None, 498)               1         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 512)               255488    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               131328    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 152)               39064     \n",
            "=================================================================\n",
            "Total params: 425,881\n",
            "Trainable params: 425,880\n",
            "Non-trainable params: 1\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1Hr9D0O1Tw0"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "We will train our model using the binary cross-entropy loss. This is because the labels are not disjoint. For a given abstract, we may have multiple categories. So, we will divide the prediction task into a series of multiple binary classification problems. This is also why we kept the activation function of the classification layer in our model to sigmoid. Researchers have used other combinations of loss function and activation function as well. For example, in [Exploring the Limits of Weakly Supervised Pretraining](https://arxiv.org/abs/1805.00932), Mahajan et al. used the softmax activation function and cross-entropy loss to train their models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCoaRkA-wsC3",
        "outputId": "2c256de2-b7b9-4274-88d8-a27257a193db"
      },
      "source": [
        "epochs = 20\n",
        "\n",
        "shallow_mlp_model.compile(\n",
        "    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"]\n",
        ")\n",
        "\n",
        "shallow_mlp_model.fit(train_dataset, validation_data=validation_dataset, epochs=epochs)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "258/258 [==============================] - 5s 11ms/step - loss: 0.0673 - categorical_accuracy: 0.6165 - val_loss: 0.0223 - val_categorical_accuracy: 0.6770\n",
            "Epoch 2/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0222 - categorical_accuracy: 0.6860 - val_loss: 0.0213 - val_categorical_accuracy: 0.6792\n",
            "Epoch 3/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0213 - categorical_accuracy: 0.6891 - val_loss: 0.0209 - val_categorical_accuracy: 0.6776\n",
            "Epoch 4/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0207 - categorical_accuracy: 0.6912 - val_loss: 0.0206 - val_categorical_accuracy: 0.6787\n",
            "Epoch 5/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0202 - categorical_accuracy: 0.6941 - val_loss: 0.0204 - val_categorical_accuracy: 0.6798\n",
            "Epoch 6/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0196 - categorical_accuracy: 0.6960 - val_loss: 0.0201 - val_categorical_accuracy: 0.6858\n",
            "Epoch 7/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0192 - categorical_accuracy: 0.6976 - val_loss: 0.0201 - val_categorical_accuracy: 0.6869\n",
            "Epoch 8/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0188 - categorical_accuracy: 0.6994 - val_loss: 0.0200 - val_categorical_accuracy: 0.6879\n",
            "Epoch 9/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0184 - categorical_accuracy: 0.7011 - val_loss: 0.0200 - val_categorical_accuracy: 0.6912\n",
            "Epoch 10/20\n",
            "258/258 [==============================] - 3s 10ms/step - loss: 0.0181 - categorical_accuracy: 0.7033 - val_loss: 0.0204 - val_categorical_accuracy: 0.6792\n",
            "Epoch 11/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0178 - categorical_accuracy: 0.7041 - val_loss: 0.0203 - val_categorical_accuracy: 0.6792\n",
            "Epoch 12/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0174 - categorical_accuracy: 0.7062 - val_loss: 0.0205 - val_categorical_accuracy: 0.6792\n",
            "Epoch 13/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0171 - categorical_accuracy: 0.7078 - val_loss: 0.0206 - val_categorical_accuracy: 0.6716\n",
            "Epoch 14/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0168 - categorical_accuracy: 0.7087 - val_loss: 0.0204 - val_categorical_accuracy: 0.6901\n",
            "Epoch 15/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0164 - categorical_accuracy: 0.7101 - val_loss: 0.0208 - val_categorical_accuracy: 0.6830\n",
            "Epoch 16/20\n",
            "258/258 [==============================] - 3s 10ms/step - loss: 0.0162 - categorical_accuracy: 0.7118 - val_loss: 0.0210 - val_categorical_accuracy: 0.6858\n",
            "Epoch 17/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0159 - categorical_accuracy: 0.7122 - val_loss: 0.0205 - val_categorical_accuracy: 0.6923\n",
            "Epoch 18/20\n",
            "258/258 [==============================] - 3s 10ms/step - loss: 0.0156 - categorical_accuracy: 0.7130 - val_loss: 0.0204 - val_categorical_accuracy: 0.6956\n",
            "Epoch 19/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0153 - categorical_accuracy: 0.7144 - val_loss: 0.0206 - val_categorical_accuracy: 0.6972\n",
            "Epoch 20/20\n",
            "258/258 [==============================] - 3s 11ms/step - loss: 0.0149 - categorical_accuracy: 0.7162 - val_loss: 0.0208 - val_categorical_accuracy: 0.7005\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0002d3810>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBvqeuk88G9r"
      },
      "source": [
        "## Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxz8yDaT8MdL",
        "outputId": "d28a3efb-d52f-4f48-8127-e729c92c225c"
      },
      "source": [
        "_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)\n",
        "print(f\"Categorical accuracy on the test set: {round(categorical_acc * 100, 2)}%.\")"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - 0s 23ms/step - loss: 0.0203 - categorical_accuracy: 0.6967\n",
            "Categorical accuracy on the test set: 69.67%.\n"
          ]
        }
      ]
    }
  ]
}